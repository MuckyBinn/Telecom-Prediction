{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flook\\AppData\\Local\\Temp\\ipykernel_17284\\2194735111.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].median(), inplace=True)\n",
      "c:\\Users\\flook\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4877 - val_accuracy: 0.7799 - val_loss: 0.4444 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7897 - loss: 0.4387 - val_accuracy: 0.7879 - val_loss: 0.4399 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7988 - loss: 0.4260 - val_accuracy: 0.7862 - val_loss: 0.4384 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8028 - loss: 0.4235 - val_accuracy: 0.7924 - val_loss: 0.4366 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8012 - loss: 0.4213 - val_accuracy: 0.7862 - val_loss: 0.4410 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8059 - loss: 0.4116 - val_accuracy: 0.7897 - val_loss: 0.4381 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m125/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 819us/step - accuracy: 0.8057 - loss: 0.4135\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8061 - loss: 0.4150 - val_accuracy: 0.7853 - val_loss: 0.4386 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8072 - loss: 0.4041 - val_accuracy: 0.7870 - val_loss: 0.4373 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8138 - loss: 0.4061 - val_accuracy: 0.7906 - val_loss: 0.4385 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m127/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.8080 - loss: 0.4073\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8087 - loss: 0.4050 - val_accuracy: 0.7897 - val_loss: 0.4367 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.7899\n",
      "ROC-AUC:       0.8348\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      1035\n",
      "           1       0.62      0.53      0.57       374\n",
      "\n",
      "    accuracy                           0.79      1409\n",
      "   macro avg       0.73      0.71      0.72      1409\n",
      "weighted avg       0.78      0.79      0.78      1409\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[914 121]\n",
      " [175 199]]\n",
      "\n",
      "✅ Saved files: churn_model.h5, scaler.pkl, feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import json\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "# Convert TotalCharges to numeric\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Fill missing values\n",
    "df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].median(), inplace=True)\n",
    "\n",
    "# Convert categorical variables\n",
    "df['gender'] = df['gender'].map({'Female': 0, 'Male': 1})\n",
    "df['Partner'] = df['Partner'].map({'No': 0, 'Yes': 1})\n",
    "df['Dependents'] = df['Dependents'].map({'No': 0, 'Yes': 1})\n",
    "df['PhoneService'] = df['PhoneService'].map({'No': 0, 'Yes': 1})\n",
    "df['PaperlessBilling'] = df['PaperlessBilling'].map({'No': 0, 'Yes': 1})\n",
    "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# One-hot encoding for categorical variables - ลบ Contract และ PaymentMethod ออก\n",
    "categorical_columns = ['InternetService', 'MultipleLines', 'OnlineSecurity',\n",
    "                      'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# Select features for model - ไม่รวม Contract และ PaymentMethod\n",
    "feature_columns = [col for col in df_encoded.columns if col != 'Churn' and col != 'customerID']\n",
    "\n",
    "# Save feature columns for later use\n",
    "with open('feature_columns.json', 'w') as f:\n",
    "    json.dump(feature_columns, f)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['Churn']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler for later use\n",
    "import pickle\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(feature_columns),)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f'\\nTest Accuracy: {test_accuracy:.4f}')\n",
    "print(f'ROC-AUC:       {roc_auc:.4f}\\n')\n",
    "\n",
    "# Print classification report\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save the model\n",
    "model.save('churn_model.h5')\n",
    "\n",
    "print(\"\\n✅ Saved files: churn_model.h5, scaler.pkl, feature_columns.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
